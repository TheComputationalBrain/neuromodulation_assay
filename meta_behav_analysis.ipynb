{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1432784/1728546854.py:25: FutureWarning: In the future `np.bool` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, 'bool'):\n",
      "/tmp/ipykernel_1432784/1728546854.py:27: FutureWarning: In the future `np.object` will be defined as the corresponding NumPy scalar.\n",
      "  if not hasattr(np, 'object'):\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import re\n",
    "import types\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import math\n",
    "import sys\n",
    "from scipy.stats import sem, ttest_1samp\n",
    "from scipy.stats import bootstrap\n",
    "from scipy.stats import linregress \n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statistics import mean\n",
    "import statsmodels.api as sm\n",
    "import scipy \n",
    "from scipy.stats import bootstrap\n",
    "\n",
    "# --- Patch deprecated NumPy aliases ---\n",
    "if not hasattr(np, 'float'):\n",
    "    np.float = float\n",
    "if not hasattr(np, 'int'):\n",
    "    np.int = int\n",
    "if not hasattr(np, 'bool'):\n",
    "    np.bool = bool\n",
    "if not hasattr(np, 'object'):\n",
    "    np.object = object\n",
    "\n",
    "# --- Patch missing pandas index types for old pickles ---\n",
    "fake_numeric = types.ModuleType(\"pandas.core.indexes.numeric\")\n",
    "fake_numeric.Int64Index = type(pd.Index([1, 2, 3]))\n",
    "fake_numeric.RangeIndex = type(pd.RangeIndex(3))\n",
    "sys.modules['pandas.core.indexes.numeric'] = fake_numeric\n",
    "\n",
    "'''import tools from the toolbox'''\n",
    "# Append the path of the main directory in the search paths for modules\n",
    "root = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "root_model = os.path.join(root, \"TransitionProbModel/\")\n",
    "if root_model not in sys.path:\n",
    "    sys.path.append(root_model)\n",
    "from TransitionProbModel.MarkovModel_Python import IdealObserver as IO\n",
    "from TransitionProbModel.MarkovModel_Python import IdealObserver as IO\n",
    "from TransitionProbModel.MarkovModel_Python import GenerateSequence as sg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(p):\n",
    "    p = np.clip(p, np.finfo(np.float).resolution, 1-np.finfo(np.float).resolution) \n",
    "    return -p*np.log2(p)-(1-p)*np.log2(1-p)\n",
    "\n",
    "\n",
    "def function_indice_change_point(list, val):\n",
    "    if len(list) < 1:#if the list is empty\n",
    "        l = [val]\n",
    "    elif min(list) >= val:\n",
    "        l = [val]\n",
    "    else :\n",
    "        l = [list[i] for i in range (len(list)) if list[i] <= val]\n",
    "    return(max(l))\n",
    "\n",
    "def print_stat_results(df, col, file, label=''):\n",
    "    collumn = pd.to_numeric(df[col], errors='coerce').dropna().to_numpy(dtype=float)\n",
    "    t_res = ttest_1samp(collumn, 0)\n",
    "    file.write(f\"{label}{col}: rho={df[col].mean():.02f}, \" +\n",
    "                f\"SD={df[col].std():.03f}, \" +\n",
    "                f\"SEM={df[col].sem():.03f}, \" +\n",
    "                f\"d={df[col].mean() / df[col].std():.03f}, \" +\n",
    "                f\"t({df[col].count()-1})={t_res.statistic:.01f}, \" +\n",
    "                f\"p={t_res.pvalue:.1e}\")\n",
    "    file.write('\\n\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1432784/4259114099.py:152: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  data = pd.concat([data, sub_data])\n"
     ]
    }
   ],
   "source": [
    "behavior_path = '/neurospin/unicog/protocols/comportement/ConfidenceDataBase_2020_Meyniel/subjects_data'\n",
    "out_dir = '/home_local/alice_hodapp/NeuroModAssay/domain_general/behavior' \n",
    "\n",
    "#subject to skip \n",
    "naconf_numbers = [3, 5, 6, 9, 36, 51]\n",
    "encodeprob_numbers = [1, 4, 12, 20]\n",
    "\n",
    "#create a dataframe \n",
    "data = pd.DataFrame(\n",
    "    columns=['subject', 'sub_pred', 'sub_conf', 'io_pred', 'io_conf',\n",
    "             'io_surp', 'io_entropy', 'res_sub_conf', 'samples_conf'])\n",
    "\n",
    "#open all subjects information dataframe\n",
    "\n",
    "filename_info = '/neurospin/unicog/protocols/comportement/ConfidenceDataBase_2020_Meyniel/all_subjects_info.pickle'\n",
    "file_info = open(filename_info, 'rb')\n",
    "all_subjects_info = pickle.load(file_info, encoding='bytes')\n",
    "\n",
    "#list of the names of all the subjects \n",
    "list_subject = os.listdir(behavior_path)\n",
    "\n",
    "valid_tasks = [\"EncodeProb2020\", \"NACONFfMRI\", \"PNAS2017\"]\n",
    "\n",
    "# Define the datasets to keep\n",
    "filtered_list_subject = [\n",
    "    subject for subject in list_subject\n",
    "    if any(valid_task in subject for valid_task in valid_tasks)  # Keep subjects that match valid values\n",
    "    and not (\n",
    "        # Extract valid task (first part before first _) and number (part after last _ and before .pickle)\n",
    "        (\n",
    "            # For \"NACONFfMRI\" task, extract number and check if it matches with naconf_numbers\n",
    "            (\"NACONFfMRI\" in subject and \"NACONFfMRI\" in subject.split('_')[0] and\n",
    "            int(subject.split('_')[-1].replace('.pickle', '')) in naconf_numbers) or\n",
    "            \n",
    "            # For \"EncodeProb\" task, extract number and check if it matches with encodeprob_numbers\n",
    "            (\"EncodeProb\" in subject and \"EncodeProb\" in subject.split('_')[0] and\n",
    "            int(subject.split('_')[-1].replace('.pickle', '')) in encodeprob_numbers)\n",
    "        )\n",
    "    )  # Skip subjects based on the number condition\n",
    "]\n",
    "\n",
    "for subject in filtered_list_subject:    \n",
    "    # Extract the number after the last underscore\n",
    "    match = re.search(r'_(\\d{3})\\.pickle$', subject)\n",
    "    if match:\n",
    "        number = int(match.group(1))  # Convert the captured number to an integer\n",
    "    \n",
    "    with open(os.path.join(behavior_path, subject), 'rb') as f:\n",
    "        sub = pickle.load(f, encoding='bytes')\n",
    "    \n",
    "    #IO estimates\n",
    "    io_pred, io_conf, io_surp, io_entropy = [], [], [], []\n",
    "    question_indices = {}\n",
    "    out_hmm = {}\n",
    "    \n",
    "    number_of_samples = []\n",
    "    generative_p = {}\n",
    "    indices_chunk = {}\n",
    "    \n",
    "    #define the structure (order) of the sequence for the given subject\n",
    "    if 'markov' in all_subjects_info.loc[all_subjects_info['sub_id'] == subject.replace('.pickle', '')]['structure'].values[0]:\n",
    "        order_prob = 1\n",
    "    elif 'bernoulli' in all_subjects_info.loc[all_subjects_info['sub_id'] == subject.replace('.pickle', '')]['structure'].values[0]:\n",
    "        order_prob = 0\n",
    "    \n",
    "    #iterate over the sessions \n",
    "    for (k,session) in enumerate(sub):\n",
    "        question_indices[k] = session['question_indices']\n",
    "        question_indices[k] = question_indices[k].astype(int)\n",
    "        if question_indices[k][-1] >= len(session['sequence']): \n",
    "            question_indices[k] = question_indices[k][:-1]\n",
    "        \n",
    "        #compute IO \n",
    "        options = {'p_c': session['volatility'], 'resol': 20}\n",
    "        out_hmm[k] = IO.IdealObserver(session['sequence'], 'hmm', order=order_prob, options=options)\n",
    "        \n",
    "        #compute IO predictions, confidences, surprises \n",
    "        for q_index in question_indices[k]:\n",
    "            if order_prob == 0: \n",
    "                io_pred.append(out_hmm[k][(1,)]['mean'][q_index])\n",
    "                io_conf.append(- np.log(out_hmm[k][(1,)]['SD'][q_index]))\n",
    "                io_surp.append(out_hmm[k]['surprise'][q_index]) \n",
    "            if order_prob == 1:         \n",
    "                if session['sequence'][q_index] == 0:\n",
    "                    io_pred.append(out_hmm[k][(0,1)]['mean'][q_index])\n",
    "                    io_conf.append(- np.log(out_hmm[k][(0,1)]['SD'][q_index]))\n",
    "                    io_surp.append(out_hmm[k]['surprise'][q_index])\n",
    "                else :\n",
    "                    io_pred.append(out_hmm[k][(1,1)]['mean'][q_index])\n",
    "                    io_conf.append(- np.log(out_hmm[k][(1,1)]['SD'][q_index]))\n",
    "                    io_surp.append(out_hmm[k]['surprise'][q_index])\n",
    "            \n",
    "        #compute number of samples since the last change point\n",
    "        generative_p[k] = session['generative p(1)']# list of generative probabilities for a session \n",
    "        indices = [] \n",
    "        for p in range (len(generative_p[k])-1):\n",
    "            if generative_p[k][p] != generative_p[k][p+1]:\n",
    "                indices.append(p)\n",
    "        indices_chunk[k]=indices\n",
    "                \n",
    "        #count number of observations since the last change point      \n",
    "        for element in question_indices[k]:\n",
    "            indice_change_point = function_indice_change_point((indices_chunk[k]), element)\n",
    "            #indices_chunk is a dict with keys = number of sessions and values for each key = chunks' indices for each session \n",
    "            #indice_change_point is the indice of the last change point before before the question indice OR the question indice itself  \n",
    "            if element != indice_change_point:\n",
    "                nb_observations = element - indice_change_point \n",
    "            else : \n",
    "                nb_observations = element + 1\n",
    "            number_of_samples.append(np.log(nb_observations))\n",
    "        \n",
    "        #convert indices into float for practical purposes              \n",
    "        samples = [float(i) for i in number_of_samples]\n",
    "    \n",
    "    \n",
    "    #compute entropy based on IO probability estimates \n",
    "    for proba_io in io_pred :\n",
    "        io_entropy.append(entropy(proba_io))  \n",
    "\n",
    "    n_questions = len(io_pred)               \n",
    "\n",
    "    if 'PNAS' in subject:\n",
    "        sub_pred = nan_array = np.nan * np.ones(n_questions) #MarkovGuess did not have a a probbaility report \n",
    "    else:\n",
    "        sub_pred = np.hstack([session['prediction_1'].tolist() for session in sub])  #prediction_1 is the next item being 1\n",
    "\n",
    "    #create a dataframe for the subject            \n",
    "    sub_data = pd.DataFrame({\n",
    "    'subject': [subject]*n_questions,\n",
    "    'sub_pred': sub_pred,  \n",
    "    'sub_conf': np.hstack([session['confidence'].tolist() for session in sub]),\n",
    "    'io_pred': np.array(io_pred),\n",
    "    'io_conf': np.array(io_conf),\n",
    "    'io_surp': np.array(io_surp),\n",
    "    'io_entropy': np.array(io_entropy),\n",
    "    'res_sub_conf': [np.nan]*n_questions,\n",
    "    'samples_conf': np.array(samples)\n",
    "    })\n",
    "        \n",
    "    #compute residuals     \n",
    "    #drop rows with NaN in relavant collumns\n",
    "    sub_data = sub_data.dropna(subset=['io_surp', 'io_entropy', 'sub_conf'])\n",
    "    sub_data.reset_index(inplace = True)\n",
    "    \n",
    "    X = np.vstack([sub_data['io_surp'].values,\n",
    "                               sub_data['io_entropy'].values,\n",
    "                               sub_data['samples_conf'].values]).T\n",
    "    reg = LinearRegression().fit(X, sub_data['sub_conf'])\n",
    "    sub_data['res_sub_conf'] = sub_data['sub_conf'] - reg.predict(X) \n",
    "    \n",
    "    # concatenate dataframes \n",
    "    data = pd.concat([data, sub_data])   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NACONFfMRI_sub_029.pickle\n",
      "PNAS2017_sub_003.pickle\n",
      "NACONFfMRI_sub_013.pickle\n",
      "NACONFfMRI_sub_007.pickle\n",
      "EncodeProb2020_sub_027.pickle\n",
      "PNAS2017_sub_020.pickle\n",
      "NACONFfMRI_sub_002.pickle\n",
      "PNAS2017_sub_021.pickle\n",
      "PNAS2017_sub_018.pickle\n",
      "EncodeProb2020_sub_025.pickle\n",
      "NACONFfMRI_sub_045.pickle\n",
      "NACONFfMRI_sub_027.pickle\n",
      "EncodeProb2020_sub_005.pickle\n",
      "EncodeProb2020_sub_030.pickle\n",
      "NACONFfMRI_sub_047.pickle\n",
      "PNAS2017_sub_002.pickle\n",
      "EncodeProb2020_sub_014.pickle\n",
      "PNAS2017_sub_013.pickle\n",
      "NACONFfMRI_sub_052.pickle\n",
      "PNAS2017_sub_007.pickle\n",
      "EncodeProb2020_sub_006.pickle\n",
      "NACONFfMRI_sub_020.pickle\n",
      "NACONFfMRI_sub_056.pickle\n",
      "PNAS2017_sub_019.pickle\n",
      "NACONFfMRI_sub_059.pickle\n",
      "PNAS2017_sub_008.pickle\n",
      "NACONFfMRI_sub_008.pickle\n",
      "PNAS2017_sub_016.pickle\n",
      "EncodeProb2020_sub_008.pickle\n",
      "NACONFfMRI_sub_049.pickle\n",
      "NACONFfMRI_sub_057.pickle\n",
      "NACONFfMRI_sub_014.pickle\n",
      "EncodeProb2020_sub_017.pickle\n",
      "PNAS2017_sub_015.pickle\n",
      "PNAS2017_sub_010.pickle\n",
      "NACONFfMRI_sub_042.pickle\n",
      "NACONFfMRI_sub_037.pickle\n",
      "NACONFfMRI_sub_041.pickle\n",
      "EncodeProb2020_sub_019.pickle\n",
      "NACONFfMRI_sub_017.pickle\n",
      "NACONFfMRI_sub_018.pickle\n",
      "NACONFfMRI_sub_039.pickle\n",
      "NACONFfMRI_sub_016.pickle\n",
      "EncodeProb2020_sub_002.pickle\n",
      "EncodeProb2020_sub_023.pickle\n",
      "NACONFfMRI_sub_040.pickle\n",
      "NACONFfMRI_sub_021.pickle\n",
      "NACONFfMRI_sub_044.pickle\n",
      "EncodeProb2020_sub_028.pickle\n",
      "NACONFfMRI_sub_030.pickle\n",
      "EncodeProb2020_sub_022.pickle\n",
      "PNAS2017_sub_011.pickle\n",
      "NACONFfMRI_sub_048.pickle\n",
      "NACONFfMRI_sub_026.pickle\n",
      "NACONFfMRI_sub_053.pickle\n",
      "NACONFfMRI_sub_004.pickle\n",
      "NACONFfMRI_sub_043.pickle\n",
      "PNAS2017_sub_009.pickle\n",
      "PNAS2017_sub_001.pickle\n",
      "NACONFfMRI_sub_024.pickle\n",
      "NACONFfMRI_sub_034.pickle\n",
      "PNAS2017_sub_017.pickle\n",
      "EncodeProb2020_sub_026.pickle\n",
      "NACONFfMRI_sub_032.pickle\n",
      "NACONFfMRI_sub_010.pickle\n",
      "NACONFfMRI_sub_001.pickle\n",
      "NACONFfMRI_sub_028.pickle\n",
      "NACONFfMRI_sub_031.pickle\n",
      "NACONFfMRI_sub_023.pickle\n",
      "EncodeProb2020_sub_015.pickle\n",
      "NACONFfMRI_sub_025.pickle\n",
      "NACONFfMRI_sub_022.pickle\n",
      "EncodeProb2020_sub_007.pickle\n",
      "EncodeProb2020_sub_003.pickle\n",
      "NACONFfMRI_sub_015.pickle\n",
      "PNAS2017_sub_006.pickle\n",
      "NACONFfMRI_sub_050.pickle\n",
      "NACONFfMRI_sub_011.pickle\n",
      "EncodeProb2020_sub_029.pickle\n",
      "PNAS2017_sub_005.pickle\n",
      "NACONFfMRI_sub_038.pickle\n",
      "PNAS2017_sub_004.pickle\n",
      "NACONFfMRI_sub_033.pickle\n",
      "NACONFfMRI_sub_060.pickle\n",
      "NACONFfMRI_sub_046.pickle\n",
      "PNAS2017_sub_014.pickle\n",
      "EncodeProb2020_sub_021.pickle\n",
      "EncodeProb2020_sub_024.pickle\n",
      "EncodeProb2020_sub_011.pickle\n",
      "NACONFfMRI_sub_019.pickle\n",
      "EncodeProb2020_sub_018.pickle\n",
      "EncodeProb2020_sub_016.pickle\n",
      "NACONFfMRI_sub_055.pickle\n",
      "PNAS2017_sub_012.pickle\n",
      "NACONFfMRI_sub_012.pickle\n",
      "NACONFfMRI_sub_035.pickle\n",
      "EncodeProb2020_sub_010.pickle\n",
      "EncodeProb2020_sub_013.pickle\n",
      "NACONFfMRI_sub_058.pickle\n",
      "EncodeProb2020_sub_009.pickle\n"
     ]
    }
   ],
   "source": [
    "subj_correlations = pd.DataFrame(index=filtered_list_subject,\n",
    "                                 columns=['sub_io_conf', 'sub_io_conf_int', 'sub_io_conf_slope',\n",
    "                                          'sub_io_pred', 'sub_io_pred_int', 'sub_io_pred_slope',\n",
    "                                          'res_sub_io_conf', 'res_sub_io_conf_int', 'res_sub_io_conf_slope', \n",
    "                                          'sub_samples_conf', 'sub_samples_conf_int', 'sub_samples_conf_slope'\n",
    "                                          ])\n",
    "for subject in filtered_list_subject:\n",
    "    print(subject)\n",
    "    sub_data = data[data['subject'] == subject]\n",
    "    for (x, y, var) in zip(['io', 'io', 'io', 'samples'],\n",
    "                           ['sub', 'res_sub', 'sub', 'sub'],\n",
    "                           ['conf', 'conf', 'pred', 'conf']):\n",
    "        \n",
    "        if (var == 'pred' and 'PNAS' in subject):\n",
    "            subj_correlations.loc[subject, \"_\".join([y, x, var])] = np.nan\n",
    "            subj_correlations.loc[subject, \"_\".join([y, x, var, 'int'])] = np.nan\n",
    "            subj_correlations.loc[subject, \"_\".join([y, x, var, 'slope'])] = np.nan\n",
    "        else:\n",
    "            clean_data = sub_data.dropna(subset=[\"_\".join([y, var])]) \n",
    "            # correlation\n",
    "            subj_correlations.loc[subject, \"_\".join([y, x, var])] = \\\n",
    "                np.corrcoef(clean_data[\"_\".join([x, var])], clean_data[\"_\".join([y, var])])[0, 1]\n",
    "\n",
    "            # linear regression\n",
    "            reg = LinearRegression().fit(\n",
    "                clean_data[\"_\".join([x, var])].to_numpy()[:, np.newaxis],  \n",
    "                clean_data[\"_\".join([y, var])].to_numpy()  \n",
    "            )\n",
    "\n",
    "            subj_correlations.loc[subject, \"_\".join([y, x, var, 'int'])] = reg.intercept_\n",
    "            subj_correlations.loc[subject, \"_\".join([y, x, var, 'slope'])] = reg.coef_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(out_dir, 'behav_group_summary_meta.txt'), \"w\") as file:\n",
    "\n",
    "    for col in ['sub_io_pred', 'sub_io_conf']:\n",
    "        print_stat_results(subj_correlations, col, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1432784/1130028820.py:17: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  binned = dataset_df.groupby(\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "agg function failed [how->mean,dtype->object]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[0;32m~/neuromodulation_assay/.venv/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:1942\u001b[0m, in \u001b[0;36mGroupBy._agg_py_fallback\u001b[0;34m(self, how, values, ndim, alt)\u001b[0m\n\u001b[1;32m   1941\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1942\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_grouper\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43magg_series\u001b[49m\u001b[43m(\u001b[49m\u001b[43mser\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpreserve_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m   1943\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/neuromodulation_assay/.venv/lib/python3.10/site-packages/pandas/core/groupby/ops.py:864\u001b[0m, in \u001b[0;36mBaseGrouper.agg_series\u001b[0;34m(self, obj, func, preserve_dtype)\u001b[0m\n\u001b[1;32m    862\u001b[0m     preserve_dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 864\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_aggregate_series_pure_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    866\u001b[0m npvalues \u001b[38;5;241m=\u001b[39m lib\u001b[38;5;241m.\u001b[39mmaybe_convert_objects(result, try_float\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/neuromodulation_assay/.venv/lib/python3.10/site-packages/pandas/core/groupby/ops.py:885\u001b[0m, in \u001b[0;36mBaseGrouper._aggregate_series_pure_python\u001b[0;34m(self, obj, func)\u001b[0m\n\u001b[1;32m    884\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(splitter):\n\u001b[0;32m--> 885\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgroup\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    886\u001b[0m     res \u001b[38;5;241m=\u001b[39m extract_result(res)\n",
      "File \u001b[0;32m~/neuromodulation_assay/.venv/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:2454\u001b[0m, in \u001b[0;36mGroupBy.mean.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   2451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   2452\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cython_agg_general(\n\u001b[1;32m   2453\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m-> 2454\u001b[0m         alt\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m   2455\u001b[0m         numeric_only\u001b[38;5;241m=\u001b[39mnumeric_only,\n\u001b[1;32m   2456\u001b[0m     )\n\u001b[1;32m   2457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/neuromodulation_assay/.venv/lib/python3.10/site-packages/pandas/core/series.py:6549\u001b[0m, in \u001b[0;36mSeries.mean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m   6541\u001b[0m \u001b[38;5;129m@doc\u001b[39m(make_doc(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m\"\u001b[39m, ndim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m   6542\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[1;32m   6543\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6547\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   6548\u001b[0m ):\n\u001b[0;32m-> 6549\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mNDFrame\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/neuromodulation_assay/.venv/lib/python3.10/site-packages/pandas/core/generic.py:12420\u001b[0m, in \u001b[0;36mNDFrame.mean\u001b[0;34m(self, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmean\u001b[39m(\n\u001b[1;32m  12414\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m  12415\u001b[0m     axis: Axis \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m  12418\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m  12419\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Series \u001b[38;5;241m|\u001b[39m \u001b[38;5;28mfloat\u001b[39m:\n\u001b[0;32m> 12420\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_stat_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  12421\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnanops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnanmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m  12422\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/neuromodulation_assay/.venv/lib/python3.10/site-packages/pandas/core/generic.py:12377\u001b[0m, in \u001b[0;36mNDFrame._stat_function\u001b[0;34m(self, name, func, axis, skipna, numeric_only, **kwargs)\u001b[0m\n\u001b[1;32m  12375\u001b[0m validate_bool_kwarg(skipna, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mskipna\u001b[39m\u001b[38;5;124m\"\u001b[39m, none_allowed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m> 12377\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reduce\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m  12378\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\n\u001b[1;32m  12379\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/neuromodulation_assay/.venv/lib/python3.10/site-packages/pandas/core/series.py:6457\u001b[0m, in \u001b[0;36mSeries._reduce\u001b[0;34m(self, op, name, axis, skipna, numeric_only, filter_type, **kwds)\u001b[0m\n\u001b[1;32m   6453\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m   6454\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSeries.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m does not allow \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwd_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnumeric_only\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6455\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwith non-numeric dtypes.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   6456\u001b[0m     )\n\u001b[0;32m-> 6457\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelegate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/neuromodulation_assay/.venv/lib/python3.10/site-packages/pandas/core/nanops.py:147\u001b[0m, in \u001b[0;36mbottleneck_switch.__call__.<locals>.f\u001b[0;34m(values, axis, skipna, **kwds)\u001b[0m\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 147\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43malt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/neuromodulation_assay/.venv/lib/python3.10/site-packages/pandas/core/nanops.py:404\u001b[0m, in \u001b[0;36m_datetimelike_compat.<locals>.new_func\u001b[0;34m(values, axis, skipna, mask, **kwargs)\u001b[0m\n\u001b[1;32m    402\u001b[0m     mask \u001b[38;5;241m=\u001b[39m isna(values)\n\u001b[0;32m--> 404\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskipna\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskipna\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m datetimelike:\n",
      "File \u001b[0;32m~/neuromodulation_assay/.venv/lib/python3.10/site-packages/pandas/core/nanops.py:720\u001b[0m, in \u001b[0;36mnanmean\u001b[0;34m(values, axis, skipna, mask)\u001b[0m\n\u001b[1;32m    719\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39msum(axis, dtype\u001b[38;5;241m=\u001b[39mdtype_sum)\n\u001b[0;32m--> 720\u001b[0m the_sum \u001b[38;5;241m=\u001b[39m \u001b[43m_ensure_numeric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthe_sum\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m axis \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(the_sum, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mndim\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/neuromodulation_assay/.venv/lib/python3.10/site-packages/pandas/core/nanops.py:1701\u001b[0m, in \u001b[0;36m_ensure_numeric\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m   1699\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m   1700\u001b[0m     \u001b[38;5;66;03m# GH#44008, GH#36703 avoid casting e.g. strings to numeric\u001b[39;00m\n\u001b[0;32m-> 1701\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not convert string \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to numeric\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1702\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[0;31mTypeError\u001b[0m: Could not convert string 'EncodeProb2020EncodeProb2020EncodeProb2020EncodeProb2020EncodeProb2020EncodeProb2020EncodeProb2020EncodeProb2020EncodeProb2020EncodeProb2020EncodeProb2020EncodeProb2020EncodeProb2020EncodeProb2020EncodeProb2020EncodeProb2020EncodeProb2020EncodeProb2020EncodeProb2020EncodeProb2020EncodeProb2020EncodeProb2020' to numeric",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[37], line 19\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m dataset_name, dataset_df \u001b[38;5;129;01min\u001b[39;00m data\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m analysis \u001b[38;5;129;01min\u001b[39;00m analysis_list:\n\u001b[1;32m     16\u001b[0m         \u001b[38;5;66;03m# Bin per subject\u001b[39;00m\n\u001b[1;32m     17\u001b[0m         binned \u001b[38;5;241m=\u001b[39m \u001b[43mdataset_df\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroupby\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     18\u001b[0m \u001b[43m            \u001b[49m\u001b[43m[\u001b[49m\u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mqcut\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset_df\u001b[49m\u001b[43m[\u001b[49m\u001b[43manalysis\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mind_var\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrank\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmethod\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfirst\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mN_BINS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msubject\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m---> 19\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     21\u001b[0m         \u001b[38;5;66;03m# Then average across subjects within each bin\u001b[39;00m\n\u001b[1;32m     22\u001b[0m         binned_mean \u001b[38;5;241m=\u001b[39m binned\u001b[38;5;241m.\u001b[39mgroupby(level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mmean()\n",
      "File \u001b[0;32m~/neuromodulation_assay/.venv/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:2452\u001b[0m, in \u001b[0;36mGroupBy.mean\u001b[0;34m(self, numeric_only, engine, engine_kwargs)\u001b[0m\n\u001b[1;32m   2445\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_numba_agg_general(\n\u001b[1;32m   2446\u001b[0m         grouped_mean,\n\u001b[1;32m   2447\u001b[0m         executor\u001b[38;5;241m.\u001b[39mfloat_dtype_mapping,\n\u001b[1;32m   2448\u001b[0m         engine_kwargs,\n\u001b[1;32m   2449\u001b[0m         min_periods\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2450\u001b[0m     )\n\u001b[1;32m   2451\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 2452\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cython_agg_general\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2453\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmean\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2454\u001b[0m \u001b[43m        \u001b[49m\u001b[43malt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mSeries\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2455\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnumeric_only\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnumeric_only\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2456\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgroupby\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/neuromodulation_assay/.venv/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:1998\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general\u001b[0;34m(self, how, alt, numeric_only, min_count, **kwargs)\u001b[0m\n\u001b[1;32m   1995\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_agg_py_fallback(how, values, ndim\u001b[38;5;241m=\u001b[39mdata\u001b[38;5;241m.\u001b[39mndim, alt\u001b[38;5;241m=\u001b[39malt)\n\u001b[1;32m   1996\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[0;32m-> 1998\u001b[0m new_mgr \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrouped_reduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray_func\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1999\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wrap_agged_manager(new_mgr)\n\u001b[1;32m   2000\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m how \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midxmin\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124midxmax\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
      "File \u001b[0;32m~/neuromodulation_assay/.venv/lib/python3.10/site-packages/pandas/core/internals/managers.py:1469\u001b[0m, in \u001b[0;36mBlockManager.grouped_reduce\u001b[0;34m(self, func)\u001b[0m\n\u001b[1;32m   1465\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m blk\u001b[38;5;241m.\u001b[39mis_object:\n\u001b[1;32m   1466\u001b[0m     \u001b[38;5;66;03m# split on object-dtype blocks bc some columns may raise\u001b[39;00m\n\u001b[1;32m   1467\u001b[0m     \u001b[38;5;66;03m#  while others do not.\u001b[39;00m\n\u001b[1;32m   1468\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m sb \u001b[38;5;129;01min\u001b[39;00m blk\u001b[38;5;241m.\u001b[39m_split():\n\u001b[0;32m-> 1469\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[43msb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1470\u001b[0m         result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m   1471\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/neuromodulation_assay/.venv/lib/python3.10/site-packages/pandas/core/internals/blocks.py:393\u001b[0m, in \u001b[0;36mBlock.apply\u001b[0;34m(self, func, **kwargs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;129m@final\u001b[39m\n\u001b[1;32m    388\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\u001b[38;5;28mself\u001b[39m, func, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mlist\u001b[39m[Block]:\n\u001b[1;32m    389\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;124;03m    apply the function to my values; return a block if we are not\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[38;5;124;03m    one\u001b[39;00m\n\u001b[1;32m    392\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 393\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    395\u001b[0m     result \u001b[38;5;241m=\u001b[39m maybe_coerce_values(result)\n\u001b[1;32m    396\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_split_op_result(result)\n",
      "File \u001b[0;32m~/neuromodulation_assay/.venv/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:1995\u001b[0m, in \u001b[0;36mGroupBy._cython_agg_general.<locals>.array_func\u001b[0;34m(values)\u001b[0m\n\u001b[1;32m   1992\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m   1994\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m alt \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1995\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_agg_py_fallback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mndim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malt\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43malt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1996\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m~/neuromodulation_assay/.venv/lib/python3.10/site-packages/pandas/core/groupby/groupby.py:1946\u001b[0m, in \u001b[0;36mGroupBy._agg_py_fallback\u001b[0;34m(self, how, values, ndim, alt)\u001b[0m\n\u001b[1;32m   1944\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124magg function failed [how->\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhow\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m,dtype->\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mser\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1945\u001b[0m     \u001b[38;5;66;03m# preserve the kind of exception that raised\u001b[39;00m\n\u001b[0;32m-> 1946\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(err)(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   1948\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ser\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m   1949\u001b[0m     res_values \u001b[38;5;241m=\u001b[39m res_values\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mTypeError\u001b[0m: agg function failed [how->mean,dtype->object]"
     ]
    }
   ],
   "source": [
    "FONTSIZE = 20\n",
    "\n",
    "N_BINS = 6\n",
    "analysis_list = [\n",
    "    {'dep_var': 'sub_pred', 'ind_var': 'io_pred', 'reg': 'sub_io_pred',\n",
    "     'x_label': 'Ideal probability estimate', 'y_label': 'Subjective probability estimate'},\n",
    "    {'dep_var': 'sub_conf', 'ind_var': 'io_conf', 'reg': 'sub_io_conf',\n",
    "     'x_label': 'Ideal confidence \\n(log precision)', 'y_label': 'Subjective confidence'}\n",
    "    ]\n",
    "\n",
    "for analysis in analysis_list:\n",
    "    binned_data = data.groupby(by=[pd.qcut(data[analysis['ind_var']].rank(method='first'), N_BINS), 'subject']).mean()\n",
    "    binned_data = binned_data.groupby(level=analysis['ind_var'])\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "    #simple_axis(ax)\n",
    "    if analysis['dep_var'] == 'io_pred':\n",
    "        xlim = [0, 1]\n",
    "    else:\n",
    "        xlim = np.array([binned_data.mean()[analysis['ind_var']].min() - 0.1,\n",
    "                         binned_data.mean()[analysis['ind_var']].max() + 0.1])\n",
    "    if analysis['dep_var'] == 'sub_conf':\n",
    "        ls = '--'\n",
    "    else:\n",
    "        ls = '-'\n",
    "    #plot of the reg line\n",
    "    ax.plot(xlim,\n",
    "             xlim * subj_correlations[analysis['reg'] + '_slope'].mean() +\n",
    "             subj_correlations[analysis['reg'] + '_int'].mean(),\n",
    "             'k',\n",
    "             lw=3,\n",
    "             color=\"darkgrey\",\n",
    "             zorder=1,\n",
    "             ls=ls)\n",
    "    ax.errorbar(binned_data.mean()[analysis['ind_var']],\n",
    "                 binned_data.mean()[analysis['dep_var']],\n",
    "                 binned_data.sem()[analysis['dep_var']],\n",
    "                 fmt='o', capsize=8,\n",
    "                 markersize=8,\n",
    "                 color=\"black\",\n",
    "                 zorder=2)\n",
    "    ax.set_xlabel(analysis['x_label'], fontsize=FONTSIZE-2)\n",
    "    ax.set_ylabel(analysis['y_label'], fontsize=FONTSIZE-2)\n",
    "    ax.tick_params(axis='both', which='major', labelsize=FONTSIZE-4, pad=8)\n",
    "    if analysis['x_label'] == 'Ideal confidence \\n(log precision)' and analysis['y_label'] == 'Subjective confidence':\n",
    "        ax.set_ylim(0.5, 0.8)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(os.path.join(out_dir, f\"{analysis['dep_var']}_vs_{analysis['ind_var']}_group.svg\"))\n",
    "    fig.savefig(os.path.join(out_dir, f\"{analysis['dep_var']}_vs_{analysis['ind_var']}_group.png\"), dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1432784/3366271286.py:21: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  binned = dataset_df.groupby(\n",
      "/tmp/ipykernel_1432784/3366271286.py:26: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  binned_mean = binned.groupby(level=0).mean()\n",
      "/tmp/ipykernel_1432784/3366271286.py:27: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  binned_sem = binned.groupby(level=0).sem()\n",
      "/tmp/ipykernel_1432784/3366271286.py:21: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  binned = dataset_df.groupby(\n",
      "/tmp/ipykernel_1432784/3366271286.py:26: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  binned_mean = binned.groupby(level=0).mean()\n",
      "/tmp/ipykernel_1432784/3366271286.py:27: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  binned_sem = binned.groupby(level=0).sem()\n",
      "/tmp/ipykernel_1432784/3366271286.py:21: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  binned = dataset_df.groupby(\n",
      "/tmp/ipykernel_1432784/3366271286.py:26: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  binned_mean = binned.groupby(level=0).mean()\n",
      "/tmp/ipykernel_1432784/3366271286.py:27: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  binned_sem = binned.groupby(level=0).sem()\n",
      "/tmp/ipykernel_1432784/3366271286.py:21: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  binned = dataset_df.groupby(\n",
      "/tmp/ipykernel_1432784/3366271286.py:26: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  binned_mean = binned.groupby(level=0).mean()\n",
      "/tmp/ipykernel_1432784/3366271286.py:27: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  binned_sem = binned.groupby(level=0).sem()\n",
      "/tmp/ipykernel_1432784/3366271286.py:21: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  binned = dataset_df.groupby(\n",
      "/tmp/ipykernel_1432784/3366271286.py:26: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  binned_mean = binned.groupby(level=0).mean()\n",
      "/tmp/ipykernel_1432784/3366271286.py:27: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  binned_sem = binned.groupby(level=0).sem()\n",
      "/tmp/ipykernel_1432784/3366271286.py:21: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  binned = dataset_df.groupby(\n",
      "/tmp/ipykernel_1432784/3366271286.py:26: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  binned_mean = binned.groupby(level=0).mean()\n",
      "/tmp/ipykernel_1432784/3366271286.py:27: FutureWarning: The default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n",
      "  binned_sem = binned.groupby(level=0).sem()\n"
     ]
    }
   ],
   "source": [
    "FONTSIZE = 20\n",
    "N_BINS = 6\n",
    "\n",
    "analysis_list = [\n",
    "    {'dep_var': 'sub_pred', 'ind_var': 'io_pred', 'reg': 'sub_io_pred',\n",
    "     'x_label': 'Ideal probability estimate', 'y_label': 'Subjective probability estimate'},\n",
    "    {'dep_var': 'sub_conf', 'ind_var': 'io_conf', 'reg': 'sub_io_conf',\n",
    "     'x_label': 'Ideal confidence \\n(log precision)', 'y_label': 'Subjective confidence'}\n",
    "]\n",
    "\n",
    "# Get dataset names from 'subject' column (everything before the first '_')\n",
    "data['dataset'] = data['subject'].str.extract(r'(^[^_]+)')\n",
    "\n",
    "for dataset_name, dataset_df in data.groupby('dataset'):\n",
    "    for analysis in analysis_list:\n",
    "        # Skip condition for specific dataset and dep_var\n",
    "        if dataset_name == 'PNAS2017' and analysis['dep_var'] == 'io_pred':\n",
    "            continue\n",
    "        \n",
    "        # Bin per subject\n",
    "        binned = dataset_df.groupby(\n",
    "            [pd.qcut(dataset_df[analysis['ind_var']].rank(method='first'), N_BINS), 'subject']\n",
    "        ).mean(numeric_only=True)  # Ensure we only aggregate numeric columns\n",
    "        \n",
    "        # Then average across subjects within each bin\n",
    "        binned_mean = binned.groupby(level=0).mean()\n",
    "        binned_sem = binned.groupby(level=0).sem()\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(4, 4))\n",
    "        if analysis['dep_var'] == 'io_pred':\n",
    "            xlim = [0, 1]\n",
    "        else:\n",
    "            x_vals = binned_mean[analysis['ind_var']]\n",
    "            xlim = np.array([x_vals.min() - 0.1, x_vals.max() + 0.1])\n",
    "\n",
    "        ls = '--' if analysis['dep_var'] == 'sub_conf' else '-'\n",
    "\n",
    "        # Regression line: average slope/intercept across subjects in this dataset\n",
    "        subset_corrs = subj_correlations.loc[\n",
    "            subj_correlations['subject'].str.startswith(dataset_name)\n",
    "        ]\n",
    "        slope_mean = subset_corrs[analysis['reg'] + '_slope'].mean()\n",
    "        int_mean = subset_corrs[analysis['reg'] + '_int'].mean()\n",
    "\n",
    "        ax.plot(xlim, slope_mean * np.array(xlim) + int_mean,\n",
    "                lw=3, color=\"darkgrey\", zorder=1, ls=ls)\n",
    "\n",
    "        ax.errorbar(binned_mean[analysis['ind_var']],\n",
    "                    binned_mean[analysis['dep_var']],\n",
    "                    binned_sem[analysis['dep_var']],\n",
    "                    fmt='o', capsize=8, markersize=8,\n",
    "                    color=\"black\", zorder=2)\n",
    "\n",
    "        ax.set_xlabel(analysis['x_label'], fontsize=FONTSIZE - 2)\n",
    "        ax.set_ylabel(analysis['y_label'], fontsize=FONTSIZE - 2)\n",
    "        ax.tick_params(axis='both', which='major', labelsize=FONTSIZE - 4, pad=8)\n",
    "\n",
    "        if analysis['x_label'] == 'Ideal confidence \\n(log precision)' and analysis['y_label'] == 'Subjective confidence':\n",
    "            # Calculate the min and max of the error bars\n",
    "            lower_bound = binned_mean[analysis['dep_var']] - binned_sem[analysis['dep_var']]\n",
    "            upper_bound = binned_mean[analysis['dep_var']] + binned_sem[analysis['dep_var']]\n",
    "            \n",
    "            # Set ymin and ymax with 0.1 padding below the minimum and above the maximum\n",
    "            ymin = lower_bound.min() - 0.1\n",
    "            ymax = upper_bound.max() + 0.1\n",
    "            \n",
    "            # Apply the new limits\n",
    "            ax.set_ylim(ymin, ymax)\n",
    "\n",
    "        fig.tight_layout()\n",
    "        filename = f\"{dataset_name}_{analysis['dep_var']}_vs_{analysis['ind_var']}_group.svg\"\n",
    "        fig.savefig(os.path.join(out_dir, filename))\n",
    "        fig.savefig(os.path.join(out_dir, filename.replace('.svg', '.png')), dpi=300)\n",
    "        plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
